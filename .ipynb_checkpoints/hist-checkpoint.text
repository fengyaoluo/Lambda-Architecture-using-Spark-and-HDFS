raw_assessment = spark.read.format("kafka").option("kafka.bootstrap.servers", "kafka:29092").option("subscribe","assessment").option("startingOffsets", "earliest").option("endingOffsets", "latest").load()  
raw_assessment.cache()
raw_assessment.printSchema()
assessment1 = raw_assessment.select(raw_assessment.value.cast('string'))  
import sys
sys.stdout = open(sys.stdout.fileno(), mode='w', encoding='utf8', buffering=1)
import json
assessment1.rdd.map(lambda x: json.loads(x.value)).toDF().show()
from pyspark.sql import Row
extracted_commits.registerTempTable('assessment')
extracted_commits = players.rdd.map(lambda x: Row(**json.loads(x.value))).toDF()
extracted_assessment1 = assessment1.rdd.map(lambda x: Row(**json.loads(x.value))).toDF()
extracted_assessment1.show()
extracted_assessment.printSchema()
extracted_assessment1.printSchema()
extracted_assessment1.registerTempTable('assessment')
spark.sql("select sequences.questions from assessment ").show(truncate = False)
spark.sql("select sequences.questions[1] from assessment ").show(truncate = False)
spark.sql("select sequences.questions[1].id, sequences.questions[1].user_incomplete  from assessment ").show(truncate = False)
spark.sql("select sequences.questions[1].id, sequences.questions[1].user_incomplete  from assessment where id is not null").show(truncate = False)
spark.sql("select sequences.questions[1].id as id, sequences.questions[1].user_incomplete  from assessment where id is not null").show(truncate = False)
spark.sql("select sequences.questions[1].id as id, sequences.questions[1].user_incomplete  from assessment where sequences.questions[1].id is not null").show(truncate = False)
spark.sql("select sequences.questions[2].id, sequences.questions[2].user_incomplete  from assessment ").show(truncate = False)
spark.sql("select sequences.questions[2] from assessment ").show(truncate = False)
spark.sql("select sequences.questions[3] from assessment ").show(truncate = False)
spark.sql("select sequences from assessment ").show(truncate = False)
spark.sql("select sequences.questions[2] from assessment ").show(truncate = False)
extracted_assessment1.printSchema()
sqlContext.sql("SELECT sequences.key, sequences.value FROM assessment")
sqlContext.sql("SELECT sequences.key, sequences.value.key FROM assessment")
spark.sql("select user_exam_id, sequences.questions from assessment ").show(truncate = False)
questions_from_exam.write.parquet("/tmp/questions_from_exam")
questions_from_exam = spark.sql("select user_exam_id, sequences.questions from assessment ").show(truncate = False)
questions_from_exam.write.parquet("/tmp/questions_from_exam")
questions_from_exam = spark.sql("select user_exam_id, sequences.questions from assessment ")
questions_from_exam.write.parquet("/tmp/questions_from_exam")
correct_from_question_1 = spark.sql("select user_exam_id, sequences.questions[1].user_correct from assessment")
correct_from_question_1.write.parquet("/temp/correct_from_question_1")
correct_from_question_1 = spark.sql("select user_exam_id, sequences.questions[1].user_correct as correct from assessment")
correct_from_question_1.write.parquet("/temp/correct_from_question_1")
correct_from_question_1 = spark.sql("select user_exam_id, sequences.questions[1].user_correct as correct from assessment")
correct_from_question_1.write.parquet("/temp/correct_from_question_1")
correct_from_question_1.write.parquet("/temp/correct_from_question_1_answer")
spark.sql("select user_exam_id, sequences.questions[1].user_correct as correct from assessment").show()
correct_from_question_1 = spark.sql("select user_exam_id, sequences.questions[1].user_correct as correct from assessment")
correct_from_question_1.write.parquet("/temp/correct_from_question_1")
answers_from_question_1 = spark.sql("select user_exam_id, sequences.questions[1] as question_1 from assessment")
answers_from_question_1.write.parquet("/temp/answers_from_question_1")
spark.sql("select count(*) from assessment ").show()
spark.sql("select exam_name, count(*)  from assessment where exam_name = 'Learning Git' group by exam_name").show()
spark.sql("select exam_name, count(*)  from assessment group by exam_name order by count(*) ").show(truncate = False)
spark.sql("select exam_name, count(*)  from assessment group by exam_name order by count(*) desc ").show(truncate = False)
extracted_assessment1.write.parquet("/tmp/extracted_assessment")
docker-compose exec spark cat /root/.python_history
